{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhoo-git/bhoo-git/blob/main/IDL_BCI_EEG_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IDL Project: Predicting BCI_EEG signals using Deep Learning**"
      ],
      "metadata": {
        "id": "gKmE7gECKOzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb torchsummaryX --quiet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummaryX import summary\n",
        "import torchvision\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocessing as mp\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Num_Workers: {mp.cpu_count()}')\n",
        "print(\"Device: \", DEVICE)"
      ],
      "metadata": {
        "id": "kVGiRP5bYDBI",
        "outputId": "631998d5-0834-4782-8ff1-9b094bf232db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Num_Workers: 2\n",
            "Device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MaPGcjXcREnk",
        "outputId": "d48c45bb-b98b-4c02-c778-31aad1a47a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BCI_EEG_data.zip    100%[===================>]   4.84G  13.1MB/s    in 7m 14s  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOriginal Data From:\\n *https://figshare.com/articles/online_resource/Shared_data_for_exploring_training_effect_in_42_human_subjects_using_a_noninvasive_sensorimotor_rhythm-based_online_BCI/7959572*\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Get BCI_EEG Data \n",
        "!mkdir /content/bci_data\n",
        "!wget -q https://cmu.box.com/shared/static/dje4whisfwszhe2vvfphvahzr563kp2x.zip --content-disposition --show-progress\n",
        "!unzip -qo 'BCI_EEG_data.zip' -d '/content/bci_data'\n",
        "\n",
        "\"\"\"\n",
        "Original Data From:\n",
        " *https://figshare.com/articles/online_resource/Shared_data_for_exploring_training_effect_in_42_human_subjects_using_a_noninvasive_sensorimotor_rhythm-based_online_BCI/7959572*\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA**\n",
        "\n",
        "---\n",
        "...Each file includes the online results from the BCI experimentation for each run (saved in a cell variable ‘BCI_UseResults’), key parameters for the experiment (saved in a structure ‘Experiment_Parm’), key parameters for the state of the raw EEG signal (saved in a structure ‘Experimental_states’), and the raw EEG signal (saved in a variable ‘output_data’).\n",
        "\n",
        "The raw EEG signals for experiments one and two are composed of 62 channels of EEG data with a sampling frequency of 100Hz, while data for experiment three contains 64 channels of EEG sampled at 128Hz..."
      ],
      "metadata": {
        "id": "gNyTyf4vJxdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'epochs'        : 20,\n",
        "    'batch_size'    : 16,\n",
        "    'lr'            : 5e-4, # 0.1 for SGD, 5e-4 for adamW\n",
        "    'patience'      : 2,\n",
        "    'lr_decay'      : 0.1,\n",
        "    'weight_decay'  : 0.001,\n",
        "    'momentum'      : 0.9,\n",
        "    'nesterov'      : True,\n",
        "    'drop_rate'     : 0.25,\n",
        "    'label_smooth'  : 0.1,\n",
        "    'drop_path'     : 0.1\n",
        "}"
      ],
      "metadata": {
        "id": "r5bbqCLYAmAg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubjectDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, subject = 1, mode = 'train'):\n",
        "        \n",
        "        self.eegs, self.results = [], []\n",
        "        for experiment in [\"Exp1\",\"Exp2\"]:\n",
        "            dir = root + '/' + experiment\n",
        "            for session in os.listdir(dir):\n",
        "                should_load = f'Subj{subject}_' in session\n",
        "                if mode == 'train':\n",
        "                    should_load = not should_load\n",
        "                if should_load:\n",
        "                    # Load a single mat file\n",
        "                    mat = scipy.io.loadmat(dir+'/'+session)\n",
        "                    # Get raw EEG data in one session\n",
        "                    session_eeg = mat['output_data']   \n",
        "                    # Get target code in one session\n",
        "                    exp_states = dict()\n",
        "                    for _ in mat['Experimental_states']:\n",
        "                        for wrapper in _:\n",
        "                            fieldnames = wrapper.dtype.names\n",
        "                            for idx, val in enumerate(wrapper):\n",
        "                                exp_states[f'{fieldnames[idx]}'] = val.flatten()\n",
        "                    session_targetCode = exp_states['TargetCode']-1\n",
        "                    '''\n",
        "                    TODO: The different effective length of 3 sessions will cause problems for data loader.\n",
        "                    --> for now, we use a fix number 100 as the effective duration\n",
        "                    --> If this impacts the model performance, we can try pad_pack_sequence\n",
        "                    '''\n",
        "                    \n",
        "                    # Remove useless data\n",
        "                    trial_start_index = np.nonzero(np.diff(session_targetCode))[0][::2]+1\n",
        "                    trial_start_end_index = np.nonzero(np.diff(session_targetCode))[0]+1\n",
        "\n",
        "                    # Split continuous eeg data to trials\n",
        "                    eeg_split_trials = np.split(session_eeg,trial_start_end_index)[1::2]\n",
        "                    \n",
        "                    # Motor imgination starts 2s after the target shows up. Plus 0.1s for reactions time\n",
        "                    effective_start = 210\n",
        "                    \n",
        "                    # The target freezes for 1s after hitting the target, signals in this 1s is useless.\n",
        "                    # Use the min duration time - 1s as the effective end time. \n",
        "                    # Different sessions have different min duration time\n",
        "                    # min_trial_duration = np.amin(np.diff(trial_start_end_index)[::2])\n",
        "                    # effective_end = min_trial_duration - 100\n",
        "                    \n",
        "                    # Update: for now, we use a fix number 100 as the effective duration\n",
        "                    effective_end = effective_start + 100\n",
        "                    \n",
        "                    effective_eeg = []\n",
        "                    for eeg in eeg_split_trials:\n",
        "                        effective_eeg.append(eeg[effective_start:effective_end,:])\n",
        "                    \n",
        "                    # Remove useless target code\n",
        "                    session_targetCode = session_targetCode[trial_start_index]\n",
        "                    \n",
        "                    # Add the session data into the array\n",
        "                    self.eegs += (effective_eeg)\n",
        "                    self.results.append(session_targetCode)\n",
        "        \n",
        "        # Concatenate results\n",
        "        self.results = np.concatenate(self.results)    \n",
        "        self.length = len(self.results)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        eegs = np.array(self.eegs[ind]) # extra [] to make an additional dimension for channel #REMOVED\n",
        "        eegs = torch.FloatTensor(eegs)\n",
        "        results = torch.tensor(self.results[ind])\n",
        "        return eegs, results\n",
        "\n",
        "    '''\n",
        "    TODO: Discuss if we need batch-wise operations.\n",
        "    '''\n",
        "    # def collate_fn(batch):\n",
        "    #     return batch"
      ],
      "metadata": {
        "id": "QYe2eAEp2fMA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test of dataset and data loader\n",
        "root = '/content/bci_data'\n",
        "train_dataset = SubjectDataset(root, subject=2, mode = 'train')\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset, \n",
        "    num_workers = 2,\n",
        "    batch_size  = 16, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    # collate_fn  = AudioDataset.collate_fn\n",
        ")\n",
        "\n",
        "test_dataset = SubjectDataset(root, subject=2, mode = 'test')\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_dataset, \n",
        "    num_workers = 2,\n",
        "    batch_size  = 16, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    # collate_fn  = AudioDataset.collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "CTgjdYup3BwD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"No. of data         : \", train_dataset.__len__())\n",
        "print(\"No. of test data    : \", test_dataset.__len__())\n",
        "print(\"Shape of data       : \", train_dataset[0][0].shape)\n",
        "\n",
        "print(\"Train batches       : \", train_loader.__len__())\n",
        "print(\"Test batches        : \", test_loader.__len__())\n",
        "\n",
        "# Test of dataset and data loader\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "    xt, yt = x, y\n",
        "    break\n",
        "print(xt.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4y_Ya8W3Whr",
        "outputId": "3742a4ed-a4ef-4906-8b43-ab698a52d1d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of data         :  9945\n",
            "No. of test data    :  375\n",
            "Shape of data       :  torch.Size([100, 62])\n",
            "Train batches       :  622\n",
            "Test batches        :  24\n",
            "torch.Size([16, 100, 62]) torch.Size([16])\n",
            "torch.Size([16, 100, 62])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model:CustomNet (Conv1d embedder - Transformer Encoder - Classification Head)"
      ],
      "metadata": {
        "id": "nPMeAQmf3iNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "\n",
        "class LockedDropout(torch.nn.Module):\n",
        "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
        "\n",
        "    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n",
        "    their `License\n",
        "    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n",
        "\n",
        "    Args:\n",
        "        p (float): Probability of an element in the dropout mask to be zeroed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.4):\n",
        "        self.p = p\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, batch_size=config['batch_size']):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n",
        "                apply dropout too.\n",
        "        \"\"\"\n",
        "        if not self.training or not self.p:\n",
        "            return x\n",
        "        x = x.clone()\n",
        "        mask = x.new_empty(batch_size, x.size(1), requires_grad=False).bernoulli_(1 - self.p)\n",
        "        mask = mask.div_(1 - self.p)\n",
        "        mask = mask.expand_as(x)\n",
        "        return x * mask\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'p=' + str(self.p) + ')'\n",
        "\n",
        "\n",
        "class pBLSTM(torch.nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "    '''\n",
        "    def __init__(self, input_size, hidden_size, dropout):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = torch.nn.LSTM(2 * input_size, hidden_size, bidirectional=True, batch_first=True, dropout=dropout, device=DEVICE) \n",
        "        # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "\n",
        "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "        x_unpack, x_lens = pad_packed_sequence(x_packed, batch_first=True)          # TODO: Pad Packed Sequence\n",
        "        x_unpack, x_lens = self.trunc_reshape(x_unpack, x_lens)                                   # Call self.trunc_reshape()\n",
        "        x_pack = pack_padded_sequence(x_unpack, x_lens, batch_first=True, enforce_sorted=False)           # TODO: Pack Padded Sequence. What output(s) would you get?\n",
        "        out, (h_n, c_n) = self.blstm(x_pack)                                                                         # TODO: Pass the sequence through bLSTM\n",
        "        return out\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens): \n",
        "        B, T, C = x.shape                          # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        if T % 2 == 1:  x = x[:,:-1,:]\n",
        "        x = x.reshape(B, T // 2, C *2)    # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "        x_lens = x_lens // 2                    # TODO: Reduce lengths by the same downsampling factor\n",
        "        return x, x_lens  "
      ],
      "metadata": {
        "id": "eU4SvzLrKdoO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomNet(nn.Module):\n",
        "    def __init__(self, in_channels=62, hid_dim=6, n_classes=2, n_layers=5,dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedder = nn.Sequential(#PermuteBlock(),\n",
        "            nn.Linear(in_channels, in_channels*2),\n",
        "            nn.LayerNorm(in_channels*2), torch.nn.GELU(),\n",
        "            nn.Linear(in_channels*2, in_channels*4),\n",
        "            nn.LayerNorm(in_channels*4), torch.nn.GELU(),\n",
        "           )#PermuteBlock())\n",
        "\n",
        "        self.rnn = nn.LSTM(in_channels*4, hid_dim, num_layers=n_layers, batch_first=True,\n",
        "                           dropout=dropout)\n",
        "        self.class_head = nn.Sequential(\n",
        "            nn.Linear(hid_dim, n_classes),\n",
        "        )\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedder(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.class_head(x)\n",
        "        return x[:,-1,:]"
      ],
      "metadata": {
        "id": "E12pR3kG3wop"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomNet(in_channels=62, hid_dim=4, n_classes=2,n_layers=6, dropout=0.4).to(DEVICE)\n",
        "print(model)\n",
        "xt = xt.cuda()\n",
        "summary(model, xt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "q1CE_bEZAGdG",
        "outputId": "4e76ca74-160f-41ba-8b74-4b2e79f760ab"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomNet(\n",
            "  (embedder): Sequential(\n",
            "    (0): Linear(in_features=62, out_features=124, bias=True)\n",
            "    (1): LayerNorm((124,), eps=1e-05, elementwise_affine=True)\n",
            "    (2): GELU(approximate='none')\n",
            "    (3): Linear(in_features=124, out_features=248, bias=True)\n",
            "    (4): LayerNorm((248,), eps=1e-05, elementwise_affine=True)\n",
            "    (5): GELU(approximate='none')\n",
            "  )\n",
            "  (rnn): LSTM(248, 4, num_layers=6, batch_first=True, dropout=0.4)\n",
            "  (class_head): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "=======================================================================\n",
            "                       Kernel Shape    Output Shape  Params Mult-Adds\n",
            "Layer                                                                \n",
            "0_embedder.Linear_0       [62, 124]  [16, 100, 124]  7.812k    7.688k\n",
            "1_embedder.LayerNorm_1        [124]  [16, 100, 124]   248.0     124.0\n",
            "2_embedder.GELU_2                 -  [16, 100, 124]       -         -\n",
            "3_embedder.Linear_3      [124, 248]  [16, 100, 248]   31.0k   30.752k\n",
            "4_embedder.LayerNorm_4        [248]  [16, 100, 248]   496.0     248.0\n",
            "5_embedder.GELU_5                 -  [16, 100, 248]       -         -\n",
            "6_rnn                             -    [16, 100, 4]  4.864k    4.672k\n",
            "7_class_head.Linear_0        [4, 2]    [16, 100, 2]    10.0       8.0\n",
            "-----------------------------------------------------------------------\n",
            "                       Totals\n",
            "Total params           44.43k\n",
            "Trainable params       44.43k\n",
            "Non-trainable params      0.0\n",
            "Mult-Adds             43.492k\n",
            "=======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       Kernel Shape    Output Shape   Params  Mult-Adds\n",
              "Layer                                                                  \n",
              "0_embedder.Linear_0       [62, 124]  [16, 100, 124]   7812.0     7688.0\n",
              "1_embedder.LayerNorm_1        [124]  [16, 100, 124]    248.0      124.0\n",
              "2_embedder.GELU_2                 -  [16, 100, 124]      NaN        NaN\n",
              "3_embedder.Linear_3      [124, 248]  [16, 100, 248]  31000.0    30752.0\n",
              "4_embedder.LayerNorm_4        [248]  [16, 100, 248]    496.0      248.0\n",
              "5_embedder.GELU_5                 -  [16, 100, 248]      NaN        NaN\n",
              "6_rnn                             -    [16, 100, 4]   4864.0     4672.0\n",
              "7_class_head.Linear_0        [4, 2]    [16, 100, 2]     10.0        8.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00df888b-7310-4d99-a12d-e0f96a4974c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_embedder.Linear_0</th>\n",
              "      <td>[62, 124]</td>\n",
              "      <td>[16, 100, 124]</td>\n",
              "      <td>7812.0</td>\n",
              "      <td>7688.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_embedder.LayerNorm_1</th>\n",
              "      <td>[124]</td>\n",
              "      <td>[16, 100, 124]</td>\n",
              "      <td>248.0</td>\n",
              "      <td>124.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_embedder.GELU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[16, 100, 124]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_embedder.Linear_3</th>\n",
              "      <td>[124, 248]</td>\n",
              "      <td>[16, 100, 248]</td>\n",
              "      <td>31000.0</td>\n",
              "      <td>30752.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_embedder.LayerNorm_4</th>\n",
              "      <td>[248]</td>\n",
              "      <td>[16, 100, 248]</td>\n",
              "      <td>496.0</td>\n",
              "      <td>248.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_embedder.GELU_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[16, 100, 248]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_rnn</th>\n",
              "      <td>-</td>\n",
              "      <td>[16, 100, 4]</td>\n",
              "      <td>4864.0</td>\n",
              "      <td>4672.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_class_head.Linear_0</th>\n",
              "      <td>[4, 2]</td>\n",
              "      <td>[16, 100, 2]</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00df888b-7310-4d99-a12d-e0f96a4974c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00df888b-7310-4d99-a12d-e0f96a4974c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00df888b-7310-4d99-a12d-e0f96a4974c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'], weight_decay=config['weight_decay'])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr']) #, weight_decay=0.001)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(patience = 5)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "         optimizer=optimizer,\n",
        "         T_max = config['epochs'],\n",
        "         verbose=True\n",
        " )\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBqIEBdEACK4",
        "outputId": "d653aa4d-adea-4a5c-ef08-a32f40d6c507"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 5.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar \n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss  = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        \n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it! \n",
        "            outputs = model(x)\n",
        "            loss    = criterion(outputs, y)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        num_correct     += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        total_loss      += float(loss.item())\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct = num_correct,\n",
        "            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        # scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        # scaler.update() \n",
        "\n",
        "        # TODO? Depending on your choice of scheduler,\n",
        "        # You may want to call some schdulers inside the train function. What are these?\n",
        "      \n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss  = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ],
      "metadata": {
        "id": "udHHcoc3CJMW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "  \n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    num_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        \n",
        "        # Move images to device\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        \n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct)\n",
        "\n",
        "        batch_bar.update()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    return acc, total_loss"
      ],
      "metadata": {
        "id": "UQ372GHOCOMn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valacc = 0.0\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    # train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    \n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        config['epochs'],\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr))\n",
        "    \n",
        "    # val_acc, val_loss = validate(model, valid_loader, criterion)\n",
        "    val_acc, val_loss = validate(model, test_loader, criterion)\n",
        "    \n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
        "\n",
        "    # wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, \n",
        "    #            'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
        "    \n",
        "    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n",
        "    # your learning rate differently\n",
        "    # scheduler.step(val_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    #Save model in drive location if val_acc is better than best recorded val_acc\n",
        "    # if val_acc >= best_valacc:\n",
        "    #   #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
        "    #   print(\"Saving model\")\n",
        "    #   torch.save({'model_state_dict':arcfacemodel.model.state_dict(),\n",
        "    #               'optimizer_state_dict':optimizer.state_dict(),\n",
        "    #               'scheduler_state_dict':scheduler.state_dict(),\n",
        "    #               'val_acc': val_acc, \n",
        "    #               'epoch': epoch}, './checkpoint.pth')\n",
        "    #   best_valacc = val_acc\n",
        "    #   wandb.save('checkpoint.pth')\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "# run.finish()"
      ],
      "metadata": {
        "id": "npGoxEOyCPUV",
        "outputId": "a57abcd2-4f29-43d9-fb5a-03267f2f5075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20: \n",
            "Train Acc 50.0000%\t Train Loss 0.6970\t Learning Rate 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Acc 48.4375%\t Val Loss 0.6980\n",
            "Adjusting learning rate of group 0 to 2.8911e-04.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   6%|▌         | 38/622 [00:00<00:10, 55.04it/s, acc=48.5577%, loss=0.6995, lr=0.0003, num_correct=303]"
          ]
        }
      ]
    }
  ]
}